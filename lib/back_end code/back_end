/*
import base64
import os
from io import BytesIO

import cv2
import mediapipe as mp
import numpy as np
import uvicorn
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse
from fastapi.staticfiles import StaticFiles
from PIL import Image

app = FastAPI()

# Initialize MediaPipe face detection
mp_face_detection = mp.solutions.face_detection
mp_drawing = mp.solutions.drawing_utils

# Default values for glasses
update_width = 150
update_height = 50

def load_glasses_image(glasses_number):
    path = os.path.join(os.getcwd(), "glasses/")
    glasses_image = cv2.imread(f"{path}{glasses_number}.png", cv2.IMREAD_UNCHANGED)
    if glasses_image is None:
        print(f"Error: Unable to load glasses image {glasses_number}.")
        exit()
    return glasses_image

def frame_generator(frame, glasses_number):
    glasses_image = load_glasses_image(glasses_number)
    resize_glasses = cv2.resize(glasses_image, (update_width, update_height))
    s_h, s_w, _ = resize_glasses.shape

    with mp_face_detection.FaceDetection(min_detection_confidence=0.5) as face_detection:
        image = frame
        if image is None:
            print("Error: Unable to read frame.")
            return

        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        imageHeight, imageWidth, _ = image.shape

        results = face_detection.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        if results.detections:
            for detection in results.detections:
                normalizedLandmark = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.NOSE_TIP)
                pixelCoordinatesLandmark = mp_drawing._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, imageWidth, imageHeight)
                Nose_tip_x = pixelCoordinatesLandmark[0]
                Nose_tip_y = pixelCoordinatesLandmark[1]

                normalizedLandmark = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.LEFT_EAR_TRAGION)
                pixelCoordinatesLandmark = mp_drawing._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, imageWidth, imageHeight)
                Left_Ear_x = pixelCoordinatesLandmark[0]
                Left_Ear_y = pixelCoordinatesLandmark[1]

                normalizedLandmark = mp_face_detection.get_key_point(detection, mp_face_detection.FaceKeyPoint.RIGHT_EAR_TRAGION)
                pixelCoordinatesLandmark = mp_drawing._normalized_to_pixel_coordinates(normalizedLandmark.x, normalizedLandmark.y, imageWidth, imageHeight)

                Right_Ear_x = pixelCoordinatesLandmark[0]
                Right_Ear_y = pixelCoordinatesLandmark[1]

                face_width = abs(Left_Ear_x - Right_Ear_x)
                sunglass_width = face_width

                sunglass_height = int((s_h / s_w) * sunglass_width)

                glass_frame = cv2.resize(glasses_image, (sunglass_width, sunglass_height))

                y_adjust = int(sunglass_height * 1.3)
                x_adjust = int(sunglass_width * 0.47)
                pos = [Nose_tip_x - x_adjust, Nose_tip_y - y_adjust]

                pos = np.array([Nose_tip_x - x_adjust, Nose_tip_y - y_adjust])

                _, _, _, alpha_channel = cv2.split(glass_frame)
                mask = alpha_channel

                for c in range(0, 3):
                    image[pos[1]:pos[1] + sunglass_height, pos[0]:pos[0] + sunglass_width, c] = (
                            image[pos[1]:pos[1] + sunglass_height, pos[0]:pos[0] + sunglass_width, c] *
                            (1 - mask / 255.0) +
                            glass_frame[:, :, c] * (mask / 255.0)
                    )

        return image

@app.websocket("/analysis-image/{glasses_number}")
async def websocket_endpoint(websocket: WebSocket, glasses_number: int):
    await websocket.accept()
    print(f"Client connected for glasses number {glasses_number}")

    try:
        while True:
            frame_bytes = await websocket.receive_text()
            frame_bytes = base64.b64decode(frame_bytes)
            frame = Image.open(BytesIO(frame_bytes))
            frame_np = np.array(frame)
            processed_frame = frame_generator(frame_np, glasses_number)
            ret, frame_encoded = cv2.imencode('.jpg', processed_frame)
            await websocket.send_bytes(frame_encoded.tobytes())
    except WebSocketDisconnect:
        print(f"Client for glasses number {glasses_number} disconnected")

@app.get("/")
async def get():
    content = """
    <!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>WebSocket Image Transfer</title>
  </head>
  <body>
    <h1>WebSocket Image Transfer</h1>
    <img id="originalImage" />
    <select id="glassesSelect" onchange="openSelectedCamera()">
      <option value="1">Glasses 1</option>
      <option value="2">Glasses 2</option>
      <option value="3">Glasses 3</option>
       <option value="4">Glasses 4</option>
      <option value="5">Glasses 5</option>
      <option value="6">Glasses 6</option>
       <option value="7">Glasses 7</option>
      <option value="8">Glasses 8</option>
      <option value="9">Glasses 9</option>
       <option value="10">Glasses 10</option>
      <option value="11">Glasses 11</option>
      <option value="12">Glasses 12</option>
       <option value="13">Glasses 13</option>
      <option value="14">Glasses 14</option>

    </select>
    <br />
    <video id="videoElement" style="display: none" autoplay></video>

    <script>
      const $ = (q) => document.querySelector(q);
      let socket;

      function openSelectedCamera() {
        const glassesSelect = $("#glassesSelect");
        const selectedGlassesNumber = glassesSelect.value;

        if (socket) {
          socket.close();
        }

        const socketUrl = `ws://192.168.1.8:8004/analysis-image/${selectedGlassesNumber}`;
        socket = new WebSocket(socketUrl);
        socket.binaryType = "arraybuffer";

        socket.onopen = function (event) {
          console.log(`WebSocket connection established for glasses number ${selectedGlassesNumber}.`);
        };

        socket.onclose = function (event) {
          console.error(`WebSocket connection closed for glasses number ${selectedGlassesNumber}.`);
          clearInterval(video_interal);
        };

        socket.onmessage = function (event) {
          console.log("Received message from server:", event);

          const arrayBufferView = new Uint8Array(event.data);
          const blob = new Blob([arrayBufferView], { type: "image/jpeg" });
          const imageUrl = URL.createObjectURL(blob);

          $("#originalImage").src = imageUrl;
        };

        socket.onerror = function (error) {
          console.error(`WebSocket error for glasses number ${selectedGlassesNumber}:`, error);
        };

        navigator.mediaDevices
          .getUserMedia({ video: true })
          .then((stream) => {
            const video = document.getElementById("videoElement");
            video.srcObject = stream;
            video.play();
            startSending(socket, video);
          })
          .catch((err) => {
            console.error("Error accessing camera:", err);
          });
      }

      function populateCameraList() {
        const videoSelect = document.getElementById("videoSelect");
        navigator.mediaDevices
          .enumerateDevices()
          .then((devices) => {
            devices.forEach((device) => {
              if (device.kind === "videoinput") {
                const option = document.createElement("option");
                option.value = device.deviceId;
                option.text = device.label || `Camera ${videoSelect.length + 1}`;
                videoSelect.appendChild(option);
              }
            });
          })
          .catch((error) => {
            console.error("Error enumerating devices:", error);
          });
      }

      function startSending(socket, video) {
        let video_interal;
        video_interal = setInterval(() => {
          const canvas = document.createElement("canvas");
          canvas.width = video.videoWidth;
          canvas.height = video.videoHeight;
          const ctx = canvas.getContext("2d");
          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);
          const imageData = canvas.toDataURL("image/jpeg").split(",")[1];

          socket.send(imageData);
        }, 300);
      }

      window.onload = () => {
        populateCameraList();
        openSelectedCamera();
      };
    </script>
  </body>
</html>
    """
    return HTMLResponse(content=content)

if __name__ == "__main__":
    uvicorn.run(app, host="192.168.1.8", port=8004)
*/